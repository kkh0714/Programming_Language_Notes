---
title: "R Notes"
output: pdf_document
date: "2026-01-30"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
suppressMessages(library(tidyverse))
suppressMessages(library(dplyr))
```

# Basic Setup

- Review STAT6180 first lecture for R & RStudio setup and assignment for LateX (pdf) setup

- R Markdown Official Documentation: https://rmarkdown.rstudio.com/lesson-1.html?_gl=1*1rvamfj*_up*MQ..*_ga*MTQzNDI2NTk5My4xNzY5ODM3MTI3*_ga_X64JZVV9NC*czE3Njk4MzcxMjYkbzEkZzAkdDE3Njk4MzcxMjYkajYwJGwwJGgw

# Useful Commands
- `update.packages()` without arguments will check every package you have installed 
- Setting file position: Go to the top menu: Session > Set Working Directory > To Source File Location.
- Input `<-` = `Alt + -`(Windows) `Option + -`(MacOS)
- Auto-indentation: `Ctrl + I`(Windows) `Control + I`(MacOS)
- `install.packages('')`
- Run all R chunks above: `Ctrl + Alt + P`(Windows) `CMD + Option + P`(MacOS) 
- Run all R chunks in document: `Ctrl + Alt + R`(Windows) `CMD + Option + R`(MacOS) 
- Insert R chunk in RMD: `Ctrl + Alt + I`(Windows) `CMD + Option + I`(MacOS) 
- Run current R chunk: `Ctrl + Enter`(Windows) `CMD + Return`(MacOS) 
- Input `%>%` = `Ctrl + Shift + M`(Windows) `CMD + Shift + M`(MacOS)
- Comment the code: `Ctrl + Shift + C`(Windows) `CMD + Shift + C`(MacOS)

# Statistic Code
- `pnorm(Z-value, lower.tail = F)`
- `pt(t-value, df, lower.tail = F)` | `qt(percentile(i.e. 0.975), df, lower.tail = F)` 
- `head(data)` | `tail(data)`
- `subset(data, subset, select) #dplyr filter has the same function` 
  - data: The data object of interest
  - subset: Condition on which to keep observations
  - select: Condition on which columns to keep
```{r}
#Example 1: top2 = subset(top3, Country != "Canada") # Ignore Canada
#Example 2: 
# wantedCols = c("Q01_gender", "Q02_hair.cost")
# mydata = subset(rawdata, Q01_gender == 1 & Q02_hair.cost > 20, select = wantedCols)
# head(genderTVhairdat)

#Example 2(dplyr):
# rawdata %>% 
#   filter(Q01_gender == 1 & Q02_hair.cost > 20) %>%
#   select(Q01_gender, Q02_hair.cost)

#   Q01_gender Q02_hair.cost
# 1          1            30    
# 2          1            35      
# 3          1            26      
# 4          1            25      
``` 
- `par(mfrow = c(1, 2))` | `plot(data, which = 1:2)`
- `boxplot(Y ~ X1 + X2 + ..., data=data)`
- `table()` to check data balance
- `hist(data$variable)` | `ggplot(data, aes(x=#the predictor#))+geom_histogram()+theme_bw()`
- `levels(drink$sex)` check the current reference level 
- `drink$sex = relevel(drink$sex, "M")` change the reference level

--- dplyr getting mean of the samples

  - `plot(data, panel = panel.smooth)` shows scatter plot with a line in each graph
  - `library(GGally)` `ggpairs(data, lower=list(continuous="smooth_loess")` shows scatter plot with correlation value in once



Replace value:
```{r}
# 1. mutate(gender_num = ifelse(gender == "male"), 1, 0)
# 2. mutate(gender_num = factor(
#     case_when(
#       gender == "male" ~ 1,
#       gender == "female" ~ 0
#       )
#     )
#    )
```

# Statistical Knowledge
### General note
- We should be very careful about excluding observations in real studies. When this is done it needs to be reported; this is important for scientific integrity.
- Assumptions:
  1. Normality assumption / the normal distribution of the $\epsilon_i$
  2. Homogeneity of Variance assumption / the homogeneous variance of the $\epsilon_i$

### Core techniques
- Z-test
- one sample t-test
- two sample t-test 
  - When the sd are very different for the two batch of samples, we need a separate test as it violates the two sample t-test assumption (STAT6180 Week2 P.36). The rule of thumb is the ratio of the sd should be less than 2 (STAT6180 Week2 P.65). Otherwise, use Welch-Satterthwaite modification (STAT6180 Week2 P.40).
  - Use F-distribution for checking equal variances
- paired t-test 
  - One of the main features of paired t-test is that the number of the observations have to be the same for both sample, such that they can be paired --STAT6180 Week2 P.34
- Confidence intervals = estimate +- critical value * estimated s.e.(estimate)

- ANOVA (Analysis of Variance) 
  - `anova()`
  
- Two-way ANOVA (STAT6180 Week10)
  - Interaction between factors `lm(Y ~ X_A + X_B + X_A:X_B, data=data)` or `aov(Y ~ X_A*X_B, data=data)`
  - Balanced or Proportionally Balanced data:
    1. Order of the factor does **NOT** matter
  - Unbalanced data:
    1. Order of the factor **DOES** matter
  - Method:
    1. Fit a model with an interaction term. If significant, then stop
    2. Fit the model with only main effects `lm(Y ~ X_A + X_B, data=data`

- Transformation (STAT6180 Week5)
  - Transformation can apply to response or predictor variables
  - Reasons to apply transformation:
    1. Data does not always follow the requirements of the statistical inference techniques
    2. We need a way of adapting the non-valid data into data that is valid to be analysed
  - `log2()` | `log10()` | `sqrt()`
  - `log(sth + 0.01)` in case there is zero value
  - However, it makes the interpretation of the result difficult. Only use it if we can detect a significant effect with a nonparametric test, we can be confident in that effect.
  
- Linear regression (STAT6180 Week7)
  - `lm(Y ~ X, data=data)`
  - Assumptions:
    1. Residuals on Normal Q-Q plot should be linear
    2. No pattern in residuals vs fitted plot
```{r}
# par(mfrow = c(1, 2))
# 1. qqnorm(data$residuals)
# 2. plot(data$fitted, data$residuals, main = "Residuals vs Fitted",xlab = "Fitted values", ylab = "Residuals")
```
  - Positive(1), Negative(-1), or no(0) linear relationship

- Multiple regression (STAT6180 Week8)
  - `lm(Y ~ X1 + X2 + X3 +..., data=data)`
  - Assumptions:
    1. Residuals on Normal Q-Q plot should be linear
    2. No pattern in residuals vs fitted plot
    3. No pattern in residuals vs $X_i$ plot
```{r}
# 3. plot(data$X1, data$residuals)
#    plot(data$X2, data$residuals)
#    plot(data$X3, data$residuals)
```
  - **Adjusted** $R^2$ is used to penalise the number of parameters to offset the increase in $R^2$
- Multi-collinearity (STAT6180 Week8 P.43)
  - When multi-collinearity exists, remove predictor with the largest p-value first. Then, regress with the reduced model until all the predictor p-values are significant
  - Rule of thumb: correlation > 0.7 is considered "high"
  - `vif(m1)` rule of thumb: VIF > 10 indicates collinearity (STAT6175 Week4 P.42)
  
- Polynomial Regression (STAT6180 Week9)
  - `lm(Y ~ X + I(X^2) + I(X^3), data=data`
  - drop with the highest order as always. Drop when it is not significant
  
- Detect Outlier (STAT6175 Week3, STAT8111 Week1)
  - `library(brrom)` | `tidy()`, `augment()`, `glance()` 
  - **influential point** is one which influences the regression parameters
  - High leverage has the ability to be influential. Rule of Thumb: 0.2-0.5 = moderate, >0.5 = high
    - `leverage <- hatvalues(lm(Y~X, data=data))`
    - `max(leverage)`
  - DFBETA `dfbetas(m1)`
  - Cook's Distance `cooks.distance(m1)`
    - $C_i > 1$ is considered to be "large"
  
Interpretation of the results:
- multi-linear interpretation (STAT6180 Week8 P.72)
- multi-linear interpretation (STAT6180 Week9 P.35)
- multi-linear interpretation (STAT6175 Week3 P.32)

- **Response variable** (**Dependent Variable**) is the outcome you are interested in measuring or predicting.
- **Predictor variable** (**Independent Variable**) is the factor you believe affects the outcome or to make a prediction

Boxplot:
- Useful when we compare the values of multiple categorical variable with different levels

